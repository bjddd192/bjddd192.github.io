---
layout: post
title: kubeadm-ha-v1.76
date: 2017-09-29 10:41:30 +0800
description: kubeadm的kubernetes高可用集群v1.76部署 
categories: kubernetes
tags: kubeadm-ha
keywords: kubernetes kubeadm v.176
---

* content
{:toc}

# kubeadm-highavailiability

本文绝大部分内容来源于[cookeem](https://github.com/cookeem)的[kubeadm-ha](https://github.com/cookeem/kubeadm-ha/blob/master/README_CN.md)一文，非常感谢作者的分享，本人在他的分享之上探索基于kubeadm的kubernetes高可用集群v.176部署，因为部署的点比较多，因此有必要详细记录一下，给自己一个备忘，同时也分享给大家，一起完善k8s的学习，有问题可以与我联系交流。






### 部署架构

#### 概要部署架构

![ha logo](/assets/2017-09-29-kubeadm-ha-v1.76/ha.png)

> kubernetes高可用的核心架构是master的高可用，kubectl、客户端以及nodes访问load balancer实现高可用。

#### 详细部署架构

![k8s ha](/assets/2017-09-29-kubeadm-ha-v1.76/k8s-ha.png)

> kubernetes组件说明

* etcd：集群的数据中心，用于存放集群的配置以及状态信息，非常重要，如果数据丢失那么集群将无法恢复；因此高可用集群部署首先就是etcd是高可用集群；

* kube-apiserver：集群核心，集群API接口、集群各个组件通信的中枢；集群安全控制；

* kube-scheduler：集群Pod的调度中心；默认kubeadm安装情况下--leader-elect参数已经设置为true，保证master集群中只有一个kube-scheduler处于活跃状态；

* kube-controller-manager：集群状态管理器，当集群状态与期望不同时，kcm会努力让集群恢复期望状态，比如：当一个pod死掉，kcm会努力新建一个pod来恢复对应replicas set期望的状态；默认kubeadm安装情况下--leader-elect参数已经设置为true，保证master集群中只有一个kube-controller-manager处于活跃状态；

* kubelet：kubernetes node agent，负责与node上的docker engine打交道；

* kube-proxy: 每个node上一个，负责service vip到endpoint pod的流量转发，当前主要通过设置iptables规则实现。

> 负载均衡

* keepalived集群设置一个虚拟ip地址，虚拟ip地址指向master1、master2、master3。

* nginx用于master1、master2、master3的apiserver的负载均衡。外部kubectl以及nodes访问apiserver的时候就可以用过keepalived的虚拟ip(192.168.200.137)以及nginx端口(81383)访问master集群的apiserver。

#### 主机节点清单

 主机名 | IP地址 | 说明 | 组件 
 :--- | :--- | :--- | :---
 k8s-m138 | 192.168.200.138 | master节点1 | etcd、kube-apiserver、kube-scheduler、kube-proxy、kubelet、calico、keepalived、nginx、kube-dashboard
 k8s-m139 | 192.168.200.139 | master节点2 | etcd、kube-apiserver、kube-scheduler、kube-proxy、kubelet、calico、keepalived、nginx、kube-dashboard
 k8s-m140 | 192.168.200.140 | master节点3 | etcd、kube-apiserver、kube-scheduler、kube-proxy、kubelet、calico、keepalived、nginx、kube-dashboard
 无 | 192.168.200.137 | keepalived虚拟IP | 无
 zabbix-46 | 192.168.200.46 | node节点 | kubelet、kube-proxy
 oooooooo | ooooooooooo | oooooooooooooo | 占位行，用于固定网格的宽度

### 前期准备

#### 安装操作系统

```sh
$ cat /etc/redhat-release 
CentOS Linux release 7.2.1511 (Core) 
```
可以安装7.2及以上的centos版本。其他linux版本未验证，仅做参考。

---

#### 准备域名解析服务器

域名解析服务器主要用于统一规划私有网络中的路由，减少重复设置的工作量。（***当然，这个不是必须的，你可以采用自己的方式做类似的处理***）

由于本人非网络运维人员，无法接触到路由器，因此自己准备了一个容器版的域名解析路由器，安装步骤很简单，请参考：[dnsmasq](https://hub.docker.com/r/andyshinn/dnsmasq/)。

本人的域名解析服务器地址为：`192.168.200.132` ，本文中的 `down.belle.cn` `registry.eyd.com` `reg.blf1.org` 皆来源于本人自定义的域名服务。

---

#### 准备文件下载服务器

文件下载服务器主要用于存放一些安装过程中需要的资源。（***当然，这个不是必须的，你可以采用自己的方式做类似的处理***）

由于安装过程中很多资源需要翻墙下载，还有一些配置文件下载之后需要根据实际情况进行调整，因此，本人采用的方式是将这些资源统一放置在一个文件下载服务器当中，加速后续的安装进程。

搭建文件下载服务器很简单，可以采用 nginx 来实现。

```sh
$ mkdir /home/docker/sfds

$ docker run -d --name sfds -p 80:9000 \
  --env 'TZ=Asia/Shanghai' --restart=always \
  -v /home/docker/sfds/nginx.conf:/etc/nginx/nginx.conf \
  -v /usr/local/sfds:/usr/share/nginx/html \
  nginx:1.11.4
```

nginx.conf 文件内容如下：

```sh
#user  nginx;
worker_processes  1;

#error_log  /var/log/nginx/error.log warn;
pid        /var/run/nginx.pid;


events {
    worker_connections  1024;
}


http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

    #log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
    #                  '$status $body_bytes_sent "$http_referer" '
    #                  '"$http_user_agent" "$http_x_forwarded_for"';

    #access_log  /var/log/nginx/access.log  main;

    sendfile        on;
    #tcp_nopush     on;

    keepalive_timeout  65;

    #gzip  on;

    #include /etc/nginx/conf.d/*.conf;

    server 
    {
        listen        9000;             #端口
        server_name  localhost;         #服务名
        root /usr/share/nginx/html;     #显示的根索引目录   
        autoindex on;                   #开启索引功能
        autoindex_exact_size off;       #关闭计算文件确切大小（单位bytes），只显示大概大小（单位kb、mb、gb）
        autoindex_localtime on;         #显示本机时间而非 GMT 时间
    }

}
```

---

#### 修改主机名

> k8s-m138
```sh
echo k8s-m138 > /etc/hostname 
sysctl kernel.hostname=k8s-m138
su -
```

> k8s-m139
```sh
echo k8s-m139 > /etc/hostname 
sysctl kernel.hostname=k8s-m139
su -
```

> k8s-m140
```sh
echo k8s-m140 > /etc/hostname 
sysctl kernel.hostname=k8s-m140
su -
```



同时将各机器使用统一的域名解析服务器，如下：

```sh
$ cat /etc/resolv.conf 
nameserver 192.168.200.132
```

每台机器hosts增加：

```sh
192.168.200.138 k8s-m138
192.168.200.139 k8s-m139
192.168.200.140 k8s-m140 
```

并在域名解析服务器增加：

```sh
k8s-m138 192.168.200.138
k8s-m139 192.168.200.139
k8s-m140 192.168.200.140
```

---

#### 禁用 Selinux、防火墙

注意：通过运行禁用 SELinux `setenforce 0` 是必需的，以允许容器访问主机文件系统，例如pod网络请求。您必须执行此操作，直到在 kubelet 中 SELinux 支持得到改进。

```sh
$ sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config

$ sed -i 's/SELINUX=permissive/SELINUX=disabled/g' /etc/selinux/config

$ setenforce 0

$ /usr/sbin/sestatus -v
SELinux status:                 disabled
```

**注意：有时未立即生效则需要重启机器。**

```sh
$ systemctl disable firewalld && systemctl stop firewalld
```

---

#### 安装 ansible

[Ansible 2.x的安装和配置](https://blog.frognew.com/2017/04/install-ansible-2.x.html)

k8s-138 将做为控制主机(ansiblecontrol)，下面在 k8s-138 上安装Ansible。

```sh
# 配置 EPEL源：
yum -y install epel-release

# 安装ansible：
yum -y install ansible

# 查看安装版本：
ansible --version
```

在各节点上创建ansible用户，并设置该用户的密码：

useradd  -d /var/lib/ansible ansible
passwd ansible

确保各节点上ansible用户具有sudo权限：

echo "ansible ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/ansible

切换到ansible用户，生成该用户的ssh秘钥对：

su ansible
ssh-keygen

ls ~/.ssh

接下来下发密钥，将公钥id_rsa.pub拷贝到各节点：

ssh-copy-id ansible@k8s-m138
ssh-copy-id ansible@k8s-m139
ssh-copy-id ansible@k8s-m140

验证公钥已生效，可以免密拷贝文件：

cd ~/.ssh
mkdir test.txt
scp -r test.txt ansible@k8s-m138:/var/lib/ansible/
scp -r test.txt ansible@k8s-m139:/var/lib/ansible/
scp -r test.txt ansible@k8s-m140:/var/lib/ansible/

修改/etc/ansible/ansible.cfg：

sudo vim /etc/ansible/ansible.cfg
[defaults]
remote_user=ansible

修改资产清单文件/etc/ansible/hosts：

sudo vim /etc/ansible/hosts

[k8s-nodes]
k8s-m138
k8s-m139
k8s-m140

[ceph-nodes]
k8s-m138
k8s-m139
k8s-m140

我们将ansible连接和管理的主机分成了两组k8s-nodes和ceph-nodes。

下面我们使用ping模块对受管主机进行ping操作：

ansible k8s-m139 -m ping

ansible k8s-nodes -m ping

ansible ceph-nodes -m ping

使用command模块执行远程命令

ansible all -m command -a "date"

命令帮助

使用ansible-doc -l可以列出Ansible支持的模块。 使用ansible-doc 模块名称将显示模块的介绍和使用示例，例如：ansible-doc command.

[Ansible playbook入门](https://blog.frognew.com/2017/04/ansible-playbook.html)

使用playbook测试nginx

cat nginx.yaml

```yaml
---
- hosts: k8s-nodes
  tasks:
  - selinux: state=disabled
  - name: Add repository
    yum_repository:
      name: nginx
      baseurl: http://nginx.org/packages/centos/7/$basearch/
      gpgcheck: no
      enabled: yes
      description: nginx repo
  - name: Install nginx
    yum: name=nginx state=present
  - name: Copy nginx.conf
    copy: src=./nginx.conf dest=/etc/nginx/nginx.conf owner=root group=root mode=0644
    notify:
    - Restart nginx
  handlers:
  - name: Restart nginx
    systemd: name=nginx state=restarted enabled=yes
```    

cat nginx.conf 

```sh
#user  nobody;
worker_processes  1;

#error_log  logs/error.log;
#error_log  logs/error.log  notice;
#error_log  logs/error.log  info;

#pid        logs/nginx.pid;


events {
    worker_connections  1024;
}


http {
    ## Basic Settings
    include       mime.types;
    default_type  application/octet-stream;
    
    sendfile        on;
    tcp_nopush     on;
    tcp_nodelay on;

    keepalive_timeout  65;
    types_hash_max_size 2048;

    ## Logging Settings
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '"$status" $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for" '
                    '"$gzip_ratio" $request_time $bytes_sent $request_length';
    
    ## open_log_file_cache max=1000 inactive=20s valid=1m min_uses=2;
    
    ## Gzip Settings
    gzip  on;
    gzip_disable "msie6";

    include /etc/nginx/conf.d/*.conf;
    include /etc/nginx/sites-enabled/*.conf;
}
```

ansible-playbook nginx.yaml --syntax-check

ansible-playbook nginx.yaml --list-task

ansible-playbook nginx.yaml --list-hosts

ansible-playbook nginx.yaml --sudo

ansible-playbook nginx.yaml --sudo --start-at-task='Copy nginx.conf'

[Ansible role入门](https://blog.frognew.com/2017/04/ansible-role.html)

[使用Ansible安装Docker CE 17.03](https://blog.frognew.com/2017/04/ansible-install-docker-ce-1703.html)

看了role，发现有点像编程了，需要找一个 IDE，这里我选择了 Atom

[Ansible 的开发工具](https://www.w3cschool.cn/automate_with_ansible/automate_with_ansible-5l6c27p8.html)

创建roles目录

sudo mkdir -pv /etc/ansible/roles/docker/{files,tasks,templates,handlers,vars,defaults,mata} 

tree /etc/ansible/roles/docker
/etc/ansible/roles/docker
├── defaults
├── files
├── handlers
├── mata
├── tasks
├── templates
└── vars

yum repolist all

ansible-playbook -i inventories/dev/hosts deploy/deploy-docker.yml --sudo

---

#### 安装docker

ansible-playbook -i inventories/dev/hosts deploy/deploy-docker.yml --sudo

[使用Ansible安装Docker CE 17.03](https://blog.frognew.com/2017/04/ansible-install-docker-ce-1703.html)

[Get Docker CE for CentOS](https://docs.docker.com/engine/installation/linux/docker-ce/centos/)

---

#### 开启路由转发、iptables

```sh
$ echo "net.ipv4.ip_forward = 1" >> /etc/sysctl.conf

$ echo "net.bridge.bridge-nf-call-ip6tables = 1" >> /etc/sysctl.conf

$ echo "net.bridge.bridge-nf-call-iptables = 1" >> /etc/sysctl.conf

$ echo "net.bridge.bridge-nf-call-arptables = 1" >> /etc/sysctl.conf

$ /sbin/sysctl -p

$ cat /etc/sysctl.conf
```

---

#### 调整 docker 参数

```sh
# 重定义对docker0网桥 
$ vim /lib/systemd/system/docker.service
# 修改为以下内容 ExecStart=/usr/bin/dockerd --bip=192.168.1138.1/24 --ip-forward=true

# 设置镜像加速器和信任仓库
$ tee /etc/docker/daemon.json <<-'EOF'
{
  "registry-mirrors": [
    "http://7c81ea9c.m.daocloud.io"
  ],
  "insecure-registries": [
    "registry.eyd.com:5000"
  ]
}
EOF

$ cat /etc/docker/daemon.json

# 重启服务
$ systemctl daemon-reload && systemctl enable docker && systemctl restart docker && systemctl status docker
```

---

#### 下载 k8s 镜像

在安装k8s的过程中，需要拉取很多镜像，因为很多镜像源在国外，因此下载速度会比较慢，或者无法下载，导致安装失败。

因此本人通过翻墙的手段，先将镜像下载到本地。（这里大家各显神通吧，我翻墙下载也经历了耗时痛苦的过程）

在一台有代理的 docker 机器上 pull 镜像：

```sh
# 172.20.30.95:8087 为本人的代理服务器
export http_proxy="http://172.20.30.95:8087"
export https_proxy="http://172.20.30.95:8087"

docker pull quay.io_coreos_etcd_v3.1.10 quay.io/coreos/etcd:v3.1.10
docker pull quay.io_calico_node_v2.5.1 quay.io/calico/node:v2.5.1
docker pull quay.io_calico_cni_v1.10.0 quay.io/calico/cni:v1.10.0
docker pull quay.io_calico_kube-policy-controller_v0.7.0 quay.io/calico/kube-policy-controller:v0.7.0

docker pull gcr.io_google_containers_kube-controller-manager-amd64_v1.7.6 gcr.io/google_containers/kube-controller-manager-amd64:v1.7.6
docker pull gcr.io_google_containers_kube-scheduler-amd64_v1.7.6 gcr.io/google_containers/kube-scheduler-amd64:v1.7.6
docker pull gcr.io_google_containers_kube-apiserver-amd64_v1.7.6 gcr.io/google_containers/kube-apiserver-amd64:v1.7.6
docker pull gcr.io_google_containers_kube-proxy-amd64_v1.7.6 gcr.io/google_containers/kube-proxy-amd64:v1.7.6
docker pull gcr.io_google_containers_k8s-dns-sidecar-amd64_1.14.4 gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.4
docker pull gcr.io_google_containers_k8s-dns-dnsmasq-nanny-amd64_1.14.4 gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.4
docker pull gcr.io_google_containers_k8s-dns-kube-dns-amd64_1.14.4 gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.4
docker pull gcr.io_google_containers_etcd-amd64_3.0.17 gcr.io/google_containers/etcd-amd64:3.0.17
docker pull gcr.io_google_containers_pause-amd64_3.0 gcr.io/google_containers/pause-amd64:3.0
docker pull gcr.io_google_containers_kubernetes-dashboard-amd64_v1.6.3 gcr.io/google_containers/kubernetes-dashboard-amd64:v1.6.3
```

保存镜像：

```sh
docker save -o quay.io_coreos_etcd_v3.1.10 quay.io/coreos/etcd:v3.1.10
docker save -o quay.io_calico_node_v2.5.1 quay.io/calico/node:v2.5.1
docker save -o quay.io_calico_cni_v1.10.0 quay.io/calico/cni:v1.10.0
docker save -o quay.io_calico_kube-policy-controller_v0.7.0 quay.io/calico/kube-policy-controller:v0.7.0

docker save -o gcr.io_google_containers_kube-controller-manager-amd64_v1.7.6 gcr.io/google_containers/kube-controller-manager-amd64:v1.7.6
docker save -o gcr.io_google_containers_kube-scheduler-amd64_v1.7.6 gcr.io/google_containers/kube-scheduler-amd64:v1.7.6
docker save -o gcr.io_google_containers_kube-apiserver-amd64_v1.7.6 gcr.io/google_containers/kube-apiserver-amd64:v1.7.6
docker save -o gcr.io_google_containers_kube-proxy-amd64_v1.7.6 gcr.io/google_containers/kube-proxy-amd64:v1.7.6
docker save -o gcr.io_google_containers_k8s-dns-sidecar-amd64_1.14.4 gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.4
docker save -o gcr.io_google_containers_k8s-dns-dnsmasq-nanny-amd64_1.14.4 gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.4
docker save -o gcr.io_google_containers_k8s-dns-kube-dns-amd64_1.14.4 gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.4
docker save -o gcr.io_google_containers_etcd-amd64_3.0.17 gcr.io/google_containers/etcd-amd64:3.0.17
docker save -o gcr.io_google_containers_pause-amd64_3.0 gcr.io/google_containers/pause-amd64:3.0
docker save -o gcr.io_google_containers_kubernetes-dashboard-amd64_v1.6.3 gcr.io/google_containers/kubernetes-dashboard-amd64:v1.6.3
```

然后将保存的镜像复制到下载服务器。（使用 scp 命令，这里略过）

在各 master 服务器装载镜像：

```sh
# 下载镜像包
wget http://down.belle.cn/package/kubernetes/v1.7.6/images/quay.io_coreos_etcd_v3.1.10
wget http://down.belle.cn/package/kubernetes/v1.7.6/images/quay.io_calico_node_v2.5.1
wget http://down.belle.cn/package/kubernetes/v1.7.6/images/quay.io_calico_cni_v1.10.0
wget http://down.belle.cn/package/kubernetes/v1.7.6/images/quay.io_calico_kube-policy-controller_v0.7.0

wget http://down.belle.cn/package/kubernetes/v1.7.6/images/gcr.io_google_containers_k8s-dns-kube-dns-amd64_1.14.4
wget http://down.belle.cn/package/kubernetes/v1.7.6/images/gcr.io_google_containers_kube-controller-manager-amd64_v1.7.6 
wget http://down.belle.cn/package/kubernetes/v1.7.6/images/gcr.io_google_containers_kube-scheduler-amd64_v1.7.6
wget http://down.belle.cn/package/kubernetes/v1.7.6/images/gcr.io_google_containers_kube-apiserver-amd64_v1.7.6
wget http://down.belle.cn/package/kubernetes/v1.7.6/images/gcr.io_google_containers_kube-proxy-amd64_v1.7.6
wget http://down.belle.cn/package/kubernetes/v1.7.6/images/gcr.io_google_containers_k8s-dns-sidecar-amd64_1.14.4
wget http://down.belle.cn/package/kubernetes/v1.7.6/images/gcr.io_google_containers_k8s-dns-dnsmasq-nanny-amd64_1.14.4
wget http://down.belle.cn/package/kubernetes/v1.7.6/images/gcr.io_google_containers_etcd-amd64_3.0.17
wget http://down.belle.cn/package/kubernetes/v1.7.6/images/gcr.io_google_containers_pause-amd64_3.0

# 装载镜像包
docker load --input quay.io_coreos_etcd_v3.1.10
docker load --input quay.io_calico_node_v2.5.1
docker load --input quay.io_calico_cni_v1.10.0
docker load --input quay.io_calico_kube-policy-controller_v0.7.0

docker load --input gcr.io_google_containers_kube-controller-manager-amd64_v1.7.6 
docker load --input gcr.io_google_containers_kube-scheduler-amd64_v1.7.6
docker load --input gcr.io_google_containers_kube-apiserver-amd64_v1.7.6
docker load --input gcr.io_google_containers_kube-proxy-amd64_v1.7.6
docker load --input gcr.io_google_containers_k8s-dns-sidecar-amd64_1.14.4
docker load --input gcr.io_google_containers_k8s-dns-dnsmasq-nanny-amd64_1.14.4 
docker load --input gcr.io_google_containers_k8s-dns-kube-dns-amd64_1.14.4
docker load --input gcr.io_google_containers_etcd-amd64_3.0.17
docker load --input gcr.io_google_containers_pause-amd64_3.0

# 删除镜像包
rm -rf quay.io_*
rm -rf gcr.io_google_containers_*
```

在各 node 服务器装载镜像：

```sh
# 下载镜像包
wget http://down.belle.cn/package/kubernetes/v1.7.6/images/gcr.io_google_containers_kubernetes-dashboard-amd64_v1.6.3
wget http://down.belle.cn/package/kubernetes/v1.7.6/images/quay.io_calico_node_v2.5.1
wget http://down.belle.cn/package/kubernetes/v1.7.6/images/quay.io_calico_cni_v1.10.0
wget http://down.belle.cn/package/kubernetes/v1.7.6/images/gcr.io_google_containers_kube-proxy-amd64_v1.7.6
wget http://down.belle.cn/package/kubernetes/v1.7.6/images/gcr.io_google_containers_pause-amd64_3.0 

# 装载镜像包
docker load --input gcr.io_google_containers_kubernetes-dashboard-amd64_v1.6.3
docker load --input quay.io_calico_node_v2.5.1
docker load --input quay.io_calico_cni_v1.10.0
docker load --input gcr.io_google_containers_kube-proxy-amd64_v1.7.6
docker load --input gcr.io_google_containers_pause-amd64_3.0

# 删除镜像包
rm -rf quay.io_*
rm -rf gcr.io_google_containers_*
```

---


#### 设置 kubernetes 仓库

本人采用的是翻墙安装的方式自己构建的yum仓库，先将 [https://packages.cloud.google.com/yum/](https://packages.cloud.google.com/yum/)  下面的内容下载到本地服务器。pool文件夹的内容看不到，需要在安装过程中获取。 

```sh
$ cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
#baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
baseurl=http://down.belle.cn/package/kubernetes/v1.7.6/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
#gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
#       https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
gpgkey=http://down.belle.cn/package/kubernetes/v1.7.6/doc/yum-key.gpg
       http://down.belle.cn/package/kubernetes/v1.7.6/doc/rpm-package-key.gpg
EOF
```

也可以不这么麻烦，使用阿里云的yum仓库（未验证，应该没有问题，网页可以正常访问）

```sh
$ cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
EOF
```

---

#### 安装 kubernetes 服务

```sh
$ yum install -y kubelet kubeadm kubectl kubernetes-cni

$ systemctl daemon-reload && systemctl restart kubelet

$ kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"7", GitVersion:"v1.7.5", GitCommit:"17d7182a7ccbb167074be7a87f0a68bd00d58d97", GitTreeState:"clean", BuildDate:"2017-08-31T08:56:23Z", GoVersion:"go1.8.3", Compiler:"gc", Platform:"linux/amd64"}

$ kubelet --version
Kubernetes v1.7.5

$ kubectl version
Client Version: version.Info{Major:"1", Minor:"7", GitVersion:"v1.7.5", GitCommit:"17d7182a7ccbb167074be7a87f0a68bd00d58d97", GitTreeState:"clean", BuildDate:"2017-08-31T09:14:02Z", GoVersion:"go1.8.3", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"7", GitVersion:"v1.7.6", GitCommit:"4bc5e7f9a6c25dc4c03d4d656f2cefd21540e28c", GitTreeState:"clean", BuildDate:"2017-09-14T06:36:08Z", GoVersion:"go1.8.3", Compiler:"gc", Platform:"linux/amd64"}
```

如要指定版本安装，可以采用以下方式：

```sh
$ yum search kubelet --showduplicates
$ yum install kubelet-1.7.5-0.x86_64

$ yum search kubeadm --showduplicates  
$ yum install kubeadm-1.7.5-0.x86_64

$ yum search kubernetes-cni --showduplicates
$ yum install kubernetes-cni-0.5.1-0.x86_64
```

安装docker 1.12.5版本需要设置`cgroup-driver=cgroupfs`：

```sh
$ sed -i 's/cgroup-driver=systemd/cgroup-driver=cgroupfs/g' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

$ systemctl daemon-reload && systemctl restart kubelet

# 查看kubelet service的日志
$ journalctl -t kubelet -f 
```

---

#### 下载 harbor 私仓证书

由于本人已经切换使用 harbor 私仓，并开启了 https 服务，因此，需要在各节点下载 harbor 的证书，否则在从私仓下载镜像时会报错：

x509: certificate signed by unknown authority.

```sh
mkdir -p /etc/docker/certs.d/reg.blf1.org
cd /etc/docker/certs.d/reg.blf1.org
wget http://down.belle.cn/package/harbor/ca.crt
```

---

### 部署 etcd 集群

etcd 是集群的数据中心，用于存放集群的配置以及状态信息，非常重要，如果数据丢失那么集群将无法恢复，因此需要首先部署 etcd 高可用集群。

#### TLS 密钥和证书

部署的etcd集群使用TLS证书对证书通信进行加密，并开启基于CA根证书签名的双向数字证书认证。

##### 安装 cfssl

```sh
# 安装 cfssl，使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 证书和秘钥文件

mkdir -p /opt/local/cfssl

cd /opt/local/cfssl

wget http://down.belle.cn/package/kubernetes/v1.6.2/cfssl/cfssl_linux-amd64
wget http://down.belle.cn/package/kubernetes/v1.6.2/cfssl/cfssljson_linux-amd64
wget http://down.belle.cn/package/kubernetes/v1.6.2/cfssl/cfssl-certinfo_linux-amd64

wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64

chmod +x *

mv cfssl_linux-amd64 cfssl
mv cfssljson_linux-amd64 cfssljson
mv cfssl-certinfo_linux-amd64 cfssl-certinfo

chmod +x cfssl*

# 创建 CA 证书配置
mkdir /opt/ssl

cd /opt/ssl

/opt/local/cfssl/cfssl print-defaults config > ca-config.json

# 修改文件 config.json
cat ca-config.json 
{
    "signing": {
        "default": {
            "expiry": "87600h"
        },
        "profiles": {
            "kubernetes": {
                "expiry": "87600h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "server auth",
                    "client auth"
                ]
            }
        }
    }
}

ca-config.json 中可以定义多个 profile，分别设置不同的 expiry 和 usages 等参数。如上面的 ca-config.json 中定义了名称为 kubernetes 的 profile，这个 profile 的 expiry 87600h 为 10 年，useages 中：

* signing表示此CA证书可以用于签名其他证书，ca.pem中的CA=TRUE
* server auth表示TLS Server Authentication
* client auth表示TLS Client Authentication


/opt/local/cfssl/cfssl print-defaults csr > ca-csr.json

# 修改文件 config.json csr.json
cat ca-csr.json 
{
    "CN": "kubernetes",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "LongHua",
            "ST": "ShenZhen",
            "O": "k8s",
            "OU": "System"
        }
    ]
}

# 生成 CA 证书和私钥
cd /opt/ssl/

/opt/local/cfssl/cfssl gencert -initca ca-csr.json | /opt/local/cfssl/cfssljson -bare ca

# 创建 etcd 证书
cd /opt/ssl/

vi etcd-csr.json
{
  "CN": "etcd",
  "hosts": [
    "127.0.0.1",
    "192.168.200.138",
    "192.168.200.139",
    "192.168.200.140",
    "k8s-m138",
    "k8s-m139",
    "k8s-m140"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "ShenZhen",
      "L": "LongHua",
      "O": "k8s",
      "OU": "System"
    }
  ]
}

注意上面配置hosts字段中制定授权使用该证书的IP和域名列表，因为现在要生成的证书需要被etcd集群各个节点使用，所以这里指定了各个节点的IP和hostname。

# 生成 etcd   密钥

/opt/local/cfssl/cfssl gencert -ca=/opt/ssl/ca.pem -ca-key=/opt/ssl/ca-key.pem \
  -config=/opt/ssl/ca-config.json -profile=kubernetes etcd-csr.json | /opt/local/cfssl/cfssljson -bare etcd
  
# 对生成的证书可以使用cfssl或openssl查看：

/opt/local/cfssl/cfssl-certinfo -cert etcd.pem

#将CA证书ca.pem, etcd秘钥etcd-key.pem, etcd证书etcd.pem拷贝到各节点的/etc/etcd/ssl目录中。

mkdir -p /etc/etcd/ssl

scp -r /opt/ssl/ca.pem 192.168.200.138:/etc/etcd/ssl/
scp -r /opt/ssl/ca.pem 192.168.200.139:/etc/etcd/ssl/ 
scp -r /opt/ssl/ca.pem 192.168.200.140:/etc/etcd/ssl/
scp -r /opt/ssl/etcd*.pem 192.168.200.138:/etc/etcd/ssl/
scp -r /opt/ssl/etcd*.pem 192.168.200.139:/etc/etcd/ssl/ 
scp -r /opt/ssl/etcd*.pem 192.168.200.140:/etc/etcd/ssl/ 

---


```

#### k8s etcd 集群

yum search etcd --showduplicates

yum -y install etcd-3.2.9

在每个节点上创建etcd的systemd unit文件/usr/lib/systemd/system/etcd.service，注意替换INITIAL_CLUSTER变量的值：

``` sh
export ETCD_NAME=`hostname`
export INTERNAL_IP=`hostname -i`
export INITIAL_CLUSTER="k8s-m138=https://192.168.200.138:2380,k8s-m139=https://192.168.200.139:2380,k8s-m140=https://192.168.200.140:2380"
tee > /usr/lib/systemd/system/etcd.service <<'EOF'
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target

[Service]  
Type=notify  
WorkingDirectory=/var/lib/etcd/  
EnvironmentFile=-/etc/etcd/etcd.conf
# set GOMAXPROCS to number of processors  
ExecStart=/bin/bash -c "GOMAXPROCS=$(nproc) /usr/bin/etcd --name=\"${ETCD_NAME}\" --data-dir=\"${ETCD_DATA_DIR}\" --cert-file=\"${ETCD_CERT_FILE}\" --key-file=\"${ETCD_KEY_FILE}\" --trusted-ca-file=\"${ETCD_TRUSTED_CA_FILE}\" --peer-cert-file=\"${ETCD_PEER_CERT_FILE}\" --peer-key-file=\"${ETCD_PEER_KEY_FILE}\" --peer-trusted-ca-file=\"${ETCD_PEER_TRUSTED_CA_FILE}\" --listen-client-urls=\"${ETCD_LISTEN_CLIENT_URLS}\"  --listen-peer-urls=\"${ETCD_LISTEN_PEER_URLS}\" --advertise-client-urls=\"${ETCD_ADVERTISE_CLIENT_URLS}\" --initial-cluster-token=\"${ETCD_INITIAL_CLUSTER_TOKEN}\" --initial-cluster=\"${ETCD_INITIAL_CLUSTER}\" --initial-cluster-state=\"${ETCD_INITIAL_CLUSTER_STATE}\""  
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF

tee > /etc/etcd/etcd.conf <<'EOF'
ETCD_NAME="@ETCD_NAME"
ETCD_DATA_DIR="/var/lib/etcd/"
ETCD_CERT_FILE="/etc/etcd/ssl/etcd.pem"
ETCD_KEY_FILE="/etc/etcd/ssl/etcd-key.pem"
ETCD_TRUSTED_CA_FILE="/etc/etcd/ssl/ca.pem"
ETCD_PEER_CERT_FILE="/etc/etcd/ssl/etcd.pem"
ETCD_PEER_KEY_FILE="/etc/etcd/ssl/etcd-key.pem"
ETCD_PEER_TRUSTED_CA_FILE="/etc/etcd/ssl/ca.pem"
ETCD_LISTEN_PEER_URLS="https://@INTERNAL_IP:2380"  
ETCD_LISTEN_CLIENT_URLS="https://@INTERNAL_IP:2379,http://127.0.0.1:2379"  
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://@INTERNAL_IP:2380"  
ETCD_ADVERTISE_CLIENT_URLS="https://@INTERNAL_IP:2379"  
ETCD_INITIAL_CLUSTER_STATE="new"  
ETCD_INITIAL_CLUSTER_TOKEN="k8s-etcd-cluster"  
ETCD_INITIAL_CLUSTER="@INITIAL_CLUSTER"
EOF

sed -i "s/@ETCD_NAME/$ETCD_NAME/g" /etc/etcd/etcd.conf
sed -i "s/@INTERNAL_IP/$INTERNAL_IP/g" /etc/etcd/etcd.conf
sed -i "s#@INITIAL_CLUSTER#${INITIAL_CLUSTER}#g" /etc/etcd/etcd.conf
```

systemctl daemon-reload
systemctl stop etcd

rm -rf /var/lib/etcd
mkdir -p /var/lib/etcd

 在各节点上启动etcd：

systemctl daemon-reload
systemctl enable etcd
systemctl start etcd
systemctl status etcd

在第一台etcd启动后会报错如下：
1月 05 14:08:38 k8s-m138 etcd[42908]: publish error: etcdserver: request timed out

这个没有关系，等第二台etcd起来以后就不会报错了，第三台起来后就基本没有什么错误了。

在master1、master2、master3上检查etcd启动状态：

```
$ ETCDCTL_API=2 etcdctl \
  --ca-file=/etc/etcd/ssl/ca.pem \
  --cert-file=/etc/etcd/ssl/etcd.pem \
  --key-file=/etc/etcd/ssl/etcd-key.pem \
  member list
4458e4bb2ccdf447: name=k8s-m140 peerURLs=https://192.168.200.140:2380 clientURLs=https://192.168.200.140:2379 isLeader=false
d9da2fcd06f4f7b5: name=k8s-m138 peerURLs=https://192.168.200.138:2380 clientURLs=https://192.168.200.138:2379 isLeader=true
f4645928575045c3: name=k8s-m139 peerURLs=https://192.168.200.139:2380 clientURLs=https://192.168.200.139:2379 isLeader=false

$ ETCDCTL_API=3 etcdctl \
  --cacert=/etc/etcd/ssl/ca.pem \
  --cert=/etc/etcd/ssl/etcd.pem \
  --key=/etc/etcd/ssl/etcd-key.pem \
  endpoint status --endpoints=192.168.200.138:2379,192.168.200.139:2379,192.168.200.140:2379
192.168.200.138:2379, 9fce1e7d2ae7a3b6, 3.0.17, 25 kB, true, 129, 529
192.168.200.139:2379, 31db8ed40ce6cb25, 3.0.17, 25 kB, false, 129, 529
192.168.200.140:2379, 2de54c0ebb659480, 3.0.17, 25 kB, false, 129, 529

$ etcdctl \
  --ca-file=/etc/etcd/ssl/ca.pem \
  --cert-file=/etc/etcd/ssl/etcd.pem \
  --key-file=/etc/etcd/ssl/etcd-key.pem \
  --endpoints=https://192.168.200.138:2379,https://192.168.200.139:2379,https://192.168.200.140:2379 \
  cluster-health
member 4458e4bb2ccdf447 is healthy: got healthy result from https://192.168.200.140:2379
member d9da2fcd06f4f7b5 is healthy: got healthy result from https://192.168.200.138:2379
member f4645928575045c3 is healthy: got healthy result from https://192.168.200.139:2379

$ etcdctl \
  --ca-file=/etc/etcd/ssl/ca.pem \
  --cert-file=/etc/etcd/ssl/etcd.pem \
  --key-file=/etc/etcd/ssl/etcd-key.pem \
  cluster-health
member 4458e4bb2ccdf447 is healthy: got healthy result from https://192.168.200.140:2379
member d9da2fcd06f4f7b5 is healthy: got healthy result from https://192.168.200.138:2379
member f4645928575045c3 is healthy: got healthy result from https://192.168.200.139:2379
```

##### 参考资料

[etcd 3.1 高可用集群搭建](https://blog.frognew.com/2017/04/install-etcd-cluster.html)

[快速搭建ETCD集群](http://blog.csdn.net/u013201439/article/details/78996831)

[使用Ansible部署etcd 3.2高可用集群](https://blog.frognew.com/2017/06/using-ansible-deploy-etcd-cluster.html)

[ansible-etcd3](https://github.com/erichll/ansible-etcd3)

---

### 第一台master初始化

#### Kubernetes各组件TLS证书和密钥

我们将禁用kube-apiserver的HTTP端口，启用Kubernetes集群相关组件的TLS通信和双向认证，下面将使用工具生成各组件TLS的证书和私钥。

我们将使用cfssl生成所需要的私钥和证书。 cfssl在 etcd 高可用集群搭建 中已经用过，这里直接使用。

##### CA证书和私钥

CN即Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名
O即Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组

##### 创建kube-apiserver证书和私钥

mkdir -p /opt/ssl/kubernetes-apiserver

cd /opt/ssl/kubernetes-apiserver

创建kube-apiserver证书签名请求配置apiserver-csr.json：

vi apiserver-csr.json 
{
  "CN": "kubernetes",
  "hosts": [
    "127.0.0.1",
    "192.168.200.137",
    "192.168.200.138",
    "192.168.200.139",
    "192.168.200.140",
    "10.96.0.1",
    "kubernetes",
    "kubernetes.default",
    "kubernetes.default.svc",
    "kubernetes.default.svc.cluster",
    "kubernetes.default.svc.cluster.local"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "ShenZhen",
      "L": "LongHua",
      "O": "k8s",
      "OU": "System"
    }
  ]
}

注意上面配置hosts字段中指定授权使用该证书的IP和域名列表，因为现在要生成的证书需要被Kubernetes Master集群各个节点使用，所以这里指定了各个节点的IP。 另外，我们为了实现kube-apiserver的高可用，我们将在其前面部署一个高可用的负载均衡器，Kubernetes的一些核心组件，通过这个负载均衡器和apiserver进行通信，因此这里在证书请求配置的hosts字段中还要加上这个负载均衡器的IP地址，这里是192.168.200.137。 同时还要指定集群内部kube-apiserver的多个域名和IP地址10.96.0.1(后边kube-apiserver-service-cluster-ip-range=10.96.0.0/12参数的指定网段的第一个IP)。

下面生成kube-apiserver的证书和私钥：

/opt/local/cfssl/cfssl gencert -ca=/opt/ssl/ca.pem \
  -ca-key=/opt/ssl/ca-key.pem \
  -config=/opt/ssl/ca-config.json \
  -profile=kubernetes /opt/ssl/kubernetes-apiserver/apiserver-csr.json | /opt/local/cfssl/cfssljson -bare apiserver
  
查看证书

/opt/local/cfssl/cfssl-certinfo -cert apiserver.pem 

##### 创建kubernetes-admin客户端证书和私钥

mkdir -p /opt/ssl/kubernetes-admin

cd /opt/ssl/kubernetes-admin

创建 admin 证书签名请求配置 admin-csr.json：

vi admin-csr.json 
{
  "CN": "kubernetes-admin",
  "hosts": [
    "192.168.200.138",
    "192.168.200.139",
    "192.168.200.140"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "ShenZhen",
      "L": "LongHua",
      "O": "system:masters",
      "OU": "System"
    }
  ]
}

kube-apiserver将提取CN作为客户端的用户名，这里是kubernetes-admin，将提取O作为用户所属的组，这里是system:master。 kube-apiserver预定义了一些 RBAC使用的ClusterRoleBindings，例如 cluster-admin将组system:masters与 ClusterRole cluster-admin绑定，而cluster-admin拥有访问kube-apiserver的所有权限，因此kubernetes-admin这个用户将作为集群的超级管理员。

下面生成kubernetes-admin的证书和私钥：

/opt/local/cfssl/cfssl gencert -ca=/opt/ssl/ca.pem \
  -ca-key=/opt/ssl/ca-key.pem \
  -config=/opt/ssl/ca-config.json \
  -profile=kubernetes /opt/ssl/kubernetes-admin/admin-csr.json | /opt/local/cfssl/cfssljson -bare admin
  
查看证书

/opt/local/cfssl/cfssl-certinfo -cert admin.pem

[This certificate lacks a "hosts" field](https://github.com/cloudflare/cfssl/issues/717)

##### 创建 kube-controller-manager 客户端证书和私钥

mkdir -p /opt/ssl/kubernetes-controller-manager

cd /opt/ssl/kubernetes-controller-manager

创建 kube-controller-manager 证书签名请求配置 controller-manager-csr.json：

vi controller-manager-csr.json 
{
  "CN": "system:kube-controller-manager",
  "hosts": [
    "192.168.200.138",
    "192.168.200.139",
    "192.168.200.140"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "ShenZhen",
      "L": "LongHua",
      "O": "system:kube-controller-manager",
      "OU": "System"
    }
  ]
}

kube-controller-manager 将提取CN作为客户端的用户名，这里是system:kube-controller-manager。 kube-controller-manager 预定义的 RBAC使用的ClusterRoleBindings system:kube-controller-manager将用户system:kube-controller-manager与ClusterRole system:kube-controller-manager绑定。

下面生成 kube-controller-manager 的证书和私钥：

/opt/local/cfssl/cfssl gencert -ca=/opt/ssl/ca.pem \
  -ca-key=/opt/ssl/ca-key.pem \
  -config=/opt/ssl/ca-config.json \
  -profile=kubernetes /opt/ssl/kubernetes-controller-manager/controller-manager-csr.json | /opt/local/cfssl/cfssljson -bare controller-manager
  
查看证书

/opt/local/cfssl/cfssl-certinfo -cert controller-manager.pem 

##### 创建 kube-scheduler 证书和私钥

mkdir -p /opt/ssl/kubernetes-scheduler

cd /opt/ssl/kubernetes-scheduler

创建 kube-scheduler 证书签名请求配置 scheduler-csr.json：

vi scheduler-csr.json 
{
  "CN": "system:kube-scheduler",
  "hosts": [
    "192.168.200.138",
    "192.168.200.139",
    "192.168.200.140"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "ShenZhen",
      "L": "LongHua",
      "O": "system:kube-scheduler",
      "OU": "System"
    }
  ]
}

kube-scheduler 将提取CN作为客户端的用户名，这里是system:kube-scheduler。 kube-apiserver预定义的RBAC使用的ClusterRoleBindings system:kube-scheduler将用户system:kube-scheduler与ClusterRole system:kube-scheduler绑定。

下面生成 kube-scheduler 的证书和私钥：

/opt/local/cfssl/cfssl gencert -ca=/opt/ssl/ca.pem \
  -ca-key=/opt/ssl/ca-key.pem \
  -config=/opt/ssl/ca-config.json \
  -profile=kubernetes /opt/ssl/kubernetes-scheduler/scheduler-csr.json | /opt/local/cfssl/cfssljson -bare scheduler
  
查看证书

/opt/local/cfssl/cfssl-certinfo -cert scheduler.pem 

将前面生成的 ca.pem, apiserver-key.pem, apiserver.pem, admin.pem, admin-key.pem, controller-manager.pem, controller-manager-key.pem, scheduler-key.pem, scheduler.pem拷贝到各个节点的/etc/kubernetes/pki目录下

mkdir -p /etc/kubernetes/pki

ssh-keygen

ssh-copy-id root@k8s-m138
ssh-copy-id root@k8s-m139
ssh-copy-id root@k8s-m140

scp -r /opt/ssl/ca*.pem 192.168.200.138:/etc/kubernetes/pki
scp -r /opt/ssl/ca*.pem 192.168.200.139:/etc/kubernetes/pki 
scp -r /opt/ssl/ca*.pem 192.168.200.140:/etc/kubernetes/pki
scp -r /opt/ssl/kubernetes-apiserver/apiserver*.pem 192.168.200.138:/etc/kubernetes/pki
scp -r /opt/ssl/kubernetes-apiserver/apiserver*.pem 192.168.200.139:/etc/kubernetes/pki
scp -r /opt/ssl/kubernetes-apiserver/apiserver*.pem 192.168.200.140:/etc/kubernetes/pki
scp -r /opt/ssl/kubernetes-admin/admin*.pem 192.168.200.138:/etc/kubernetes/pki
scp -r /opt/ssl/kubernetes-admin/admin*.pem 192.168.200.139:/etc/kubernetes/pki
scp -r /opt/ssl/kubernetes-admin/admin*.pem 192.168.200.140:/etc/kubernetes/pki
scp -r /opt/ssl/kubernetes-controller-manager/controller-manager*.pem 192.168.200.138:/etc/kubernetes/pki
scp -r /opt/ssl/kubernetes-controller-manager/controller-manager*.pem 192.168.200.139:/etc/kubernetes/pki
scp -r /opt/ssl/kubernetes-controller-manager/controller-manager*.pem 192.168.200.140:/etc/kubernetes/pki
scp -r /opt/ssl/kubernetes-scheduler/scheduler*.pem 192.168.200.138:/etc/kubernetes/pki
scp -r /opt/ssl/kubernetes-scheduler/scheduler*.pem 192.168.200.139:/etc/kubernetes/pki
scp -r /opt/ssl/kubernetes-scheduler/scheduler*.pem 192.168.200.140:/etc/kubernetes/pki

ll /etc/kubernetes/pki

将Kubernetes二进制包解压后kubernetes/server/bin中的kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet拷贝到各节点的/usr/local/bin目录中：

cd /tmp
tar -zxvf /tmp/kubernetes-server-linux-amd64_v1.6.2.tar.gz

scp -r /tmp/kubernetes/server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet} 192.168.200.138:/usr/local/bin/

scp -r /tmp/kubernetes/server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet} 192.168.200.139:/usr/local/bin/

scp -r /tmp/kubernetes/server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet} 192.168.200.140:/usr/local/bin/

##### 创建 kubelet 证书和私钥

```sh
mkdir -p /opt/ssl/kubelet_k8s-m138

cd /opt/ssl/kubelet_k8s-m138

# 创建访问ApiServer的证书和私钥，kubelet-csr.json：

tee kubelet-csr.json <<-'EOF'
{
  "CN": "system:node:k8s-m138",
  "hosts": [
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "ShenZhen",
      "L": "LongHua",
      "O": "system:nodes",
      "OU": "System"
    }
  ]
}
EOF

#注意CN为用户名，使用system:node:<node-name>
#O为用户组，Kubernetes RBAC定义了ClusterRoleBinding将Group system:nodes和ClusterRole system:node关联。
#生成证书和私钥：

/opt/local/cfssl/cfssl gencert -ca=/opt/ssl/ca.pem \
  -ca-key=/opt/ssl/ca-key.pem \
  -config=/opt/ssl/ca-config.json \
  -profile=kubernetes /opt/ssl/kubelet_k8s-m138/kubelet-csr.json | /opt/local/cfssl/cfssljson -bare kubelet
  
#查看证书

/opt/local/cfssl/cfssl-certinfo -cert kubelet.pem 

scp -r /opt/ssl/kubelet_k8s-m138/*.pem 192.168.200.138:/etc/kubernetes/pki
```

##### 创建 kube-proxy 证书和私钥

``` sh
mkdir -p /opt/ssl/kubernetes-proxy

cd /opt/ssl/kubernetes-proxy

# 创建 kube-proxy 证书签名请求配置 kube-proxy-csr.json：

tee kube-proxy-csr.json <<-'EOF'
{
  "CN": "system:kube-proxy",
  "hosts": [
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "ShenZhen",
      "L": "LongHua",
      "O": "system:kube-proxy",
      "OU": "System"
    }
  ]
}
EOF

# CN 指定该证书的 User为 system:kube-proxy。Kubernetes RBAC定义了ClusterRoleBinding将system:kube-proxy用户与system:node-proxier 角色绑定。system:node-proxier具有kube-proxy组件访问ApiServer的相关权限。

# 下面生成 kube-proxy 的证书和私钥：

/opt/local/cfssl/cfssl gencert -ca=/opt/ssl/ca.pem \
  -ca-key=/opt/ssl/ca-key.pem \
  -config=/opt/ssl/ca-config.json \
  -profile=kubernetes /opt/ssl/kubernetes-proxy/kube-proxy-csr.json | /opt/local/cfssl/cfssljson -bare kube-proxy
  
# 查看证书

/opt/local/cfssl/cfssl-certinfo -cert kube-proxy.pem 

# 拷贝到node节点
scp -r /opt/ssl/kubernetes-proxy/*.pem 192.168.200.138:/etc/kubernetes/pki
scp -r /opt/ssl/kubernetes-proxy/*.pem 192.168.200.139:/etc/kubernetes/pki
scp -r /opt/ssl/kubernetes-proxy/*.pem 192.168.200.140:/etc/kubernetes/pki
```

#### kube-apiserver部署

创建kube-apiserver的systemd unit文件/usr/lib/systemd/system/kube-apiserver.service，注意替换INTERNAL_IP变量的值：

```sh
mkdir -p /var/log/kubernetes
export INTERNAL_IP=`hostname -i`
export ETCD_SERVERS="https://192.168.200.138:2379,https://192.168.200.139:2379,https://192.168.200.140:2379"
tee > /usr/lib/systemd/system/kube-apiserver.service <<'EOF'
[Unit]
Description=kube-apiserver
After=network.target
After=etcd.service

[Service]
EnvironmentFile=-/etc/kubernetes/apiserver
ExecStart=/usr/local/bin/kube-apiserver \
    --service-cluster-ip-range=10.96.0.0/12 \
    --service-account-key-file=/etc/kubernetes/pki/ca-key.pem \
    --client-ca-file=/etc/kubernetes/pki/ca.pem \
    --tls-cert-file=/etc/kubernetes/pki/apiserver.pem \
    --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem \
    --insecure-port=0 \
    --secure-port=6443 \
    --logtostderr=true \
    --v=0 \
    --allow-privileged=true \
    --experimental-bootstrap-token-auth=true \
    --advertise-address=@INTERNAL_IP \
    --bind-address=@INTERNAL_IP \
    --storage-backend=etcd3 \
    --etcd-servers=@ETCD_SERVERS \
    --etcd-cafile=/etc/etcd/ssl/ca.pem \
    --etcd-certfile=/etc/etcd/ssl/etcd.pem \
    --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \
    --apiserver-count=3 \
    --enable-swagger-ui=true \
    --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds \
    --authorization-mode=RBAC \
    --audit-log-maxage=30 \
    --audit-log-maxbackup=3 \
    --audit-log-maxsize=100 \
    --audit-log-path=/var/log/kubernetes/audit.log
Restart=on-failure
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF

sed -i "s/@INTERNAL_IP/$INTERNAL_IP/g" /usr/lib/systemd/system/kube-apiserver.service
sed -i "s#@ETCD_SERVERS#${ETCD_SERVERS}#g" /usr/lib/systemd/system/kube-apiserver.service

systemctl daemon-reload
systemctl enable kube-apiserver
systemctl start kube-apiserver
systemctl status kube-apiserver
```

--insecure-port禁用了不安全的http端口
--secure-port指定https安全端口，kube-scheduler、kube-controller-manager、kubelet、kube-proxy、kubectl等组件都将使用安全端口与ApiServer通信(实际上会由我们在前端部署的负载均衡器代理)。
--authorization-mode=RBAC表示在安全端口启用RBAC授权模式，在授权过程会拒绝会授权的请求。kube-scheduler、kube-controller-manager、kubelet、kube-proxy、kubectl等组件都使用各自证书或kubeconfig指定相关的User、Group来通过RBAC授权。
--admission-control为准入机制，这里配置了NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds
--service-cluster-ip-range指定Service Cluster IP的地址段，注意该地址段是Kubernetes Service使用的，是虚拟IP，从外部不能路由可达。

#### 配置kubectl访问apiserver

我们已经部署了kube-apiserver，并且前面已经kubernetes-admin的证书和私钥，我们将使用这个用户访问ApiServer。

export KUBE_APISERVER="https://"`hostname -i`":6443"

kubectl \
    --server=${KUBE_APISERVER} \
    --certificate-authority=/etc/kubernetes/pki/ca.pem  \
    --client-certificate=/etc/kubernetes/pki/admin.pem \
    --client-key=/etc/kubernetes/pki/admin-key.pem \
    get componentstatuses
    
NAME                 STATUS      MESSAGE                                                                                        ERROR
scheduler            Unhealthy   Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: getsockopt: connection refused   
controller-manager   Unhealthy   Get http://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: getsockopt: connection refused   
etcd-2               Healthy     {"health": "true"}                                                                             
etcd-1               Healthy     {"health": "true"}                                                                             
etcd-0               Healthy     {"health": "true"}   
    
上面我们使用kubectl命令打印出了Kubernetes核心组件的状态，因为我们还没有部署kube-scheduler和controller-manager，所以这两个组件当前是不健康的。

前面面使用kubectl时需要指定ApiServer的地址以及客户端的证书，用起来比较繁琐。 接下来我们创建kubernetes-admin的kubeconfig文件 admin.conf。

```sh
cd /etc/kubernetes

export KUBE_APISERVER="https://"`hostname -i`":6443"

# set-cluster
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/pki/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=admin.conf

# set-credentials
kubectl config set-credentials kubernetes-admin \
  --client-certificate=/etc/kubernetes/pki/admin.pem \
  --embed-certs=true \
  --client-key=/etc/kubernetes/pki/admin-key.pem \
  --kubeconfig=admin.conf

# set-context
kubectl config set-context kubernetes-admin@kubernetes \
  --cluster=kubernetes \
  --user=kubernetes-admin \
  --kubeconfig=admin.conf
  
# set default context
kubectl config use-context kubernetes-admin@kubernetes --kubeconfig=admin.conf
```

为了使用kubectl访问apiserver使用admin.conf，将其拷贝到$HOME/.kube中并重命名成config：

cp /etc/kubernetes/admin.conf ~/.kube/config

尝试直接使用kubectl命令访问apiserver：

kubectl get cs

因为我们已经把kubernetes-admin用户的相关信息和证书保存到了admin.conf中，所以可以删除/etc/kubernetes/pki中的admin.pem和admin-key.pem:(可选)

cd /etc/kubernetes/pki
rm -f admin.pem admin-key.pem

经过前面的步骤，我们已经在node1,node2,node3上部署了Kubernetes Master节点的kube-apiserver，地址如下：

https://192.168.200.138:6443
https://192.168.200.139:6443
https://192.168.200.140:6443

为了实现高可用，可以在前面放一个负载均衡器来代理访问kube-apiserver的请求，再对负载均衡器实现高可用。 例如，HAProxy+Keepavlided，我们在node1~node3这3个Master节点上运行keepalived和HAProxy，3个keepalived争抢同一个VIP地址，3个HAProxy也都尝试去绑定到这个VIP的同一个端口。 例如：192.168.200:8443，当某个Haproxy出现异常后，其他节点上的Keepalived会争抢到VIP，同时这个节点上HAProxy会接管流量。

HAProxy+Keepavlided的部署这里不再展开。 最终kube-apiserver高可用的地址为https://192.168.200:8443，修改前面$HOME/.kube/config中的clusters:cluster:server的值为这个地址。

#### keepalived安装配置

在k8s-master、master2、master3上安装keepalived

```sh
yum search keepalived --showduplicates
yum -y install keepalived
```

在master1、master2、master3上设置apiserver监控脚本，当apiserver检测失败的时候关闭keepalived服务，转移虚拟IP地址

```
tee /etc/keepalived/check_apiserver.sh <<-'EOF'
#!/bin/bash

err=0

for k in $( seq 1 10 )
do
    check_code=$(ps -ef | grep kube-apiserver | wc -l)
    if [ "$check_code" = "1" ]; then
        err=$(expr $err + 1)
        sleep 3
        continue
    else
        err=0
        break
    fi
done

if [ "$err" != "0" ]; then
    echo "systemctl stop keepalived"
    /usr/bin/systemctl stop keepalived
    exit 1
else
    exit 0
fi

EOF

chmod +x /etc/keepalived/check_apiserver.sh
```

在master1、master2、master3上查看接口名字为：ens160 

```sh
$ ip a | grep 192.168.200
    inet 192.168.200.140/24 brd 192.168.200.255 scope global ens160
```

在master1上设置keepalived

```sh
export INTERNAL_IP=`hostname -i`
export INTERFACE=`ip a | grep ${INTERNAL_IP} | awk '{print $NF}'`
export VIRTUAL_IPADDRESS="192.168.200.137"

tee /etc/keepalived/keepalived.conf <<-'EOF'
! Configuration File for keepalived

global_defs {
    router_id LVS_DEVEL
}

vrrp_script chk_apiserver {
    script "/etc/keepalived/check_apiserver.sh"
    interval 2
    weight -5
    fall 3  
    rise 2
}

vrrp_instance VI_1 {
    state MASTER
    interface @INTERFACE
    mcast_src_ip @INTERNAL_IP
    virtual_router_id 78
    priority 150
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 4be37dc3b4c90194d1600c483e10ad1d
    }
    virtual_ipaddress {
        @VIRTUAL_IPADDRESS
    }
    track_script {
       chk_apiserver
    }
}
EOF

sed -i "s/@INTERFACE/$INTERFACE/g" /etc/keepalived/keepalived.conf
sed -i "s/@INTERNAL_IP/$INTERNAL_IP/g" /etc/keepalived/keepalived.conf
sed -i "s/@VIRTUAL_IPADDRESS/$VIRTUAL_IPADDRESS/g" /etc/keepalived/keepalived.conf
```

在master2上设置keepalived

```sh
export INTERNAL_IP=`hostname -i`
export INTERFACE=`ip a | grep ${INTERNAL_IP} | awk '{print $NF}'`
export VIRTUAL_IPADDRESS="192.168.200.137"

tee /etc/keepalived/keepalived.conf <<-'EOF'
! Configuration File for keepalived

global_defs {
    router_id LVS_DEVEL
}

vrrp_script chk_apiserver {
    script "/etc/keepalived/check_apiserver.sh"
    interval 2
    weight -5
    fall 3  
    rise 2
}

vrrp_instance VI_1 {
    state BACKUP
    interface @INTERFACE
    mcast_src_ip @INTERNAL_IP
    virtual_router_id 78
    priority 120
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 4be37dc3b4c90194d1600c483e10ad1d
    }
    virtual_ipaddress {
        @VIRTUAL_IPADDRESS
    }
    track_script {
       chk_apiserver
    }
}
EOF

sed -i "s/@INTERFACE/$INTERFACE/g" /etc/keepalived/keepalived.conf
sed -i "s/@INTERNAL_IP/$INTERNAL_IP/g" /etc/keepalived/keepalived.conf
sed -i "s/@VIRTUAL_IPADDRESS/$VIRTUAL_IPADDRESS/g" /etc/keepalived/keepalived.conf
```

在master3上设置keepalived

```sh
export INTERNAL_IP=`hostname -i`
export INTERFACE=`ip a | grep ${INTERNAL_IP} | awk '{print $NF}'`
export VIRTUAL_IPADDRESS="192.168.200.137"

tee /etc/keepalived/keepalived.conf <<-'EOF'
! Configuration File for keepalived

global_defs {
    router_id LVS_DEVEL
}

vrrp_script chk_apiserver {
    script "/etc/keepalived/check_apiserver.sh"
    interval 2
    weight -5
    fall 3  
    rise 2
}

vrrp_instance VI_1 {
    state BACKUP
    interface @INTERFACE
    mcast_src_ip @INTERNAL_IP
    virtual_router_id 78
    priority 100
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 4be37dc3b4c90194d1600c483e10ad1d
    }
    virtual_ipaddress {
        @VIRTUAL_IPADDRESS
    }
    track_script {
       chk_apiserver
    }
}
EOF

sed -i "s/@INTERFACE/$INTERFACE/g" /etc/keepalived/keepalived.conf
sed -i "s/@INTERNAL_IP/$INTERNAL_IP/g" /etc/keepalived/keepalived.conf
sed -i "s/@VIRTUAL_IPADDRESS/$VIRTUAL_IPADDRESS/g" /etc/keepalived/keepalived.conf
```

重启keepalived服务，并验证vip已绑定

```sh 
systemctl enable keepalived 
systemctl restart keepalived
systemctl status keepalived
ip a | grep 192.168.200
```

---

#### nginx负载均衡配置

在master1、master2、master3上通过nginx把访问apiserver的61383端口负载均衡到8433端口上：

```sh
mkdir -p /home/docker/ha/nginx

cd /home/docker/ha/nginx

tee /home/docker/ha/nginx/nginx-slb.conf <<-'EOF'   
user  nginx;
worker_processes  1;

error_log  /var/log/nginx/error.log warn;
pid        /var/run/nginx.pid;


events {
    worker_connections  65535;
}


http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';

    access_log  /var/log/nginx/access.log  main;

    sendfile        on;
    #tcp_nopush     on;

    keepalive_timeout  65;

    #gzip  on;

    include /etc/nginx/conf.d/*.conf;
}

stream {
        upstream apiserver {
            server 192.168.200.138:6443 weight=5 max_fails=3 fail_timeout=5s;
            server 192.168.200.139:6443 weight=5 max_fails=3 fail_timeout=5s;
            server 192.168.200.140:6443 weight=5 max_fails=3 fail_timeout=5s;
        }

    server {
        listen 8443;
        proxy_connect_timeout 1s;
        proxy_timeout 3s;
        proxy_pass apiserver;
    }
}
EOF

docker stop nginx-slb && docker rm nginx-slb 

docker run -d --name nginx-slb -p 8443:8443 \
    --env 'TZ=Asia/Shanghai' --restart=always \
    -v /home/docker/ha/nginx/nginx-slb.conf:/etc/nginx/nginx.conf \
    bjddd192/nginx:1.10.1
```

注意：业务恢复后务必手工重启keepalived，否则keepalived会处于关闭状态。

```
$ systemctl restart keepalived
$ systemctl status keepalived
```

#### kube-controller-manager部署

前面已经创建了controller-manager.pem和controller-manage-key.pem，下面生成controller-manager的kubeconfig文件controller-manager.conf：

```sh
cd /etc/kubernetes

export KUBE_APISERVER="https://192.168.200.137:8443"

# set-cluster
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/pki/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=controller-manager.conf

# set-credentials
kubectl config set-credentials system:kube-controller-manager \
  --client-certificate=/etc/kubernetes/pki/controller-manager.pem \
  --embed-certs=true \
  --client-key=/etc/kubernetes/pki/controller-manager-key.pem \
  --kubeconfig=controller-manager.conf

# set-context
kubectl config set-context system:kube-controller-manager@kubernetes \
  --cluster=kubernetes \
  --user=system:kube-controller-manager \
  --kubeconfig=controller-manager.conf
  
# set default context
kubectl config use-context system:kube-controller-manager@kubernetes --kubeconfig=controller-manager.conf
```

创建kube-controller-manager的systemd unit文件/usr/lib/systemd/system/kube-controller-manager.service：

```sh
export KUBE_APISERVER="https://192.168.200.137:8443"

tee > /usr/lib/systemd/system/kube-controller-manager.service <<-'EOF'
[Unit]
Description=kube-controller-manager
After=network.target
After=kube-apiserver.service

[Service]
EnvironmentFile=-/etc/kubernetes/controller-manager
ExecStart=/usr/local/bin/kube-controller-manager \
    --logtostderr=true \
    --v=0 \
    --master=@KUBE_APISERVER \
    --kubeconfig=/etc/kubernetes/controller-manager.conf \
    --cluster-name=kubernetes \
    --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem \
    --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem \
    --service-account-private-key-file=/etc/kubernetes/pki/ca-key.pem \
    --root-ca-file=/etc/kubernetes/pki/ca.pem \
    --insecure-experimental-approve-all-kubelet-csrs-for-group=system:bootstrappers \
    --use-service-account-credentials=true \
    --service-cluster-ip-range=10.96.0.0/12 \
    --cluster-cidr=10.244.0.0/16 \
    --allocate-node-cidrs=true \
    --leader-elect=true \
    --controllers=*,bootstrapsigner,tokencleaner
Restart=on-failure
Type=simple
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF

sed -i "s#@KUBE_APISERVER#$KUBE_APISERVER#g" /usr/lib/systemd/system/kube-controller-manager.service

systemctl daemon-reload
systemctl enable kube-controller-manager
systemctl stop kube-controller-manager
systemctl start kube-controller-manager
systemctl status kube-controller-manager

# 到这一步三个Master节点上的kube-controller-manager部署完成，通过选举出一个leader工作。 分别在三个节点查看状态：

systemctl status -l kube-controller-manager
```

#### kube-scheduler部署

前面已经创建了scheduler.pem和scheduler-key.pem，下面生成kube-scheduler的kubeconfig文件scheduler.conf：

```sh
cd /etc/kubernetes

export KUBE_APISERVER="https://192.168.200.137:8443"

# set-cluster
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/pki/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=scheduler.conf

# set-credentials
kubectl config set-credentials system:kube-scheduler \
  --client-certificate=/etc/kubernetes/pki/scheduler.pem \
  --embed-certs=true \
  --client-key=/etc/kubernetes/pki/scheduler-key.pem \
  --kubeconfig=scheduler.conf

# set-context
kubectl config set-context system:kube-scheduler@kubernetes \
  --cluster=kubernetes \
  --user=system:kube-scheduler \
  --kubeconfig=scheduler.conf
  
# set default context
kubectl config use-context system:kube-scheduler@kubernetes --kubeconfig=scheduler.conf
```

创建kube-scheduler的systemd unit文件/usr/lib/systemd/system/kube-scheduler.service：

```sh
export KUBE_APISERVER="https://192.168.200.137:8443"

tee > /usr/lib/systemd/system/kube-scheduler.service <<-'EOF'
[Unit]
Description=kube-scheduler
After=network.target
After=kube-apiserver.service

[Service]
EnvironmentFile=-/etc/kubernetes/scheduler
ExecStart=/usr/local/bin/kube-scheduler \
	    --logtostderr=true \
	    --v=0 \
	    --master=@KUBE_APISERVER \
	    --kubeconfig=/etc/kubernetes/scheduler.conf \
	    --leader-elect=true
Restart=on-failure
Type=simple
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF

sed -i "s#@KUBE_APISERVER#$KUBE_APISERVER#g" /usr/lib/systemd/system/kube-scheduler.service

systemctl daemon-reload
systemctl enable kube-scheduler
systemctl stop kube-scheduler
systemctl start kube-scheduler
systemctl status kube-scheduler

# 到这一步三个Master节点上的kube-scheduler部署完成，通过选举出一个leader工作。 分别在三个节点查看状态：

systemctl status -l kube-scheduler
```

### Kubernetes Node节点部署

Kubernetes的一个Node节点上需要运行如下组件：

Docker，这个我们前面在做环境准备的时候已经在各节点部署和运行了
kubelet
kube-proxy
下面我们以在node1上为例部署这些组件：

#### CNI安装

```sh
wget https://github.com/containernetworking/cni/releases/download/v0.5.2/cni-amd64-v0.5.2.tgz
mkdir -p /opt/cni/bin
tar -zxvf cni-amd64-v0.5.2.tgz -C /opt/cni/bin
ls /opt/cni/bin/
# 删除包
```

#### kubelet部署

```sh
# 创建kubelet的工作目录：

mkdir -p /var/lib/kubelet
mkdir -p /etc/kubernetes/manifests

# 安装依赖包：

yum -y install ebtables socat util-linux conntrack-tools

# 生成kubeconfig文件kubelet.conf

cd /etc/kubernetes

export HOST_NAME=`hostname`
export KUBE_APISERVER="https://192.168.200.137:8443"

# set-cluster
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/pki/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=kubelet.conf

# set-credentials
kubectl config set-credentials system:node:${HOST_NAME} \
  --client-certificate=/etc/kubernetes/pki/kubelet.pem \
  --embed-certs=true \
  --client-key=/etc/kubernetes/pki/kubelet-key.pem \
  --kubeconfig=kubelet.conf

# set-context
kubectl config set-context system:node:${HOST_NAME}@kubernetes \
  --cluster=kubernetes \
  --user=system:node:${HOST_NAME} \
  --kubeconfig=kubelet.conf
  
# set default context
kubectl config use-context system:node:${HOST_NAME}@kubernetes --kubeconfig=kubelet.conf
```

创建kubelet的systemd unit service文件，注意替换NodeIP变量：

```sh
export NODE_IP=`hostname -i`
export KUBE_APISERVER="https://192.168.200.137:8443"

tee > /usr/lib/systemd/system/kubelet.service <<-'EOF'
[Unit]
Description=kubelet
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
EnvironmentFile=-/etc/kubernetes/kubelet
ExecStart=/usr/local/bin/kubelet \
        --logtostderr=true \
        --v=0 \
        --address=@NODE_IP \
        --api-servers==@KUBE_APISERVER \
        --cluster-dns=10.96.0.10 \
        --cluster-domain=cluster.local \
        --kubeconfig=/etc/kubernetes/kubelet.conf \
        --require-kubeconfig=true \
        --pod-manifest-path=/etc/kubernetes/manifests \
        --allow-privileged=true \
        --authorization-mode=AlwaysAllow \
#        --authorization-mode=Webhook \
#        --client-ca-file=/etc/kubernetes/pki/ca.pem \
        --network-plugin=cni \
        --cni-conf-dir=/etc/cni/net.d \
        --cni-bin-dir=/opt/cni/bin
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOF

sed -i "s#@NODE_IP#$NODE_IP#g" /usr/lib/systemd/system/kubelet.service
sed -i "s#@KUBE_APISERVER#$KUBE_APISERVER#g" /usr/lib/systemd/system/kubelet.service

systemctl daemon-reload
systemctl enable kubelet
systemctl stop kubelet
systemctl start kubelet
systemctl status -l kubelet

kubectl get nodes
```

--pod-manifest-path=/etc/kubernetes/manifests指定了静态Pod定义的目录。可以提前创建好这个目录mkdir -p /etc/kubernetes/manifests。关于静态Pod的内容可参考之前写的一篇《Kubernetes资源对象之Pod》中的静态Pod的内容。
--authorization-mode=AlwaysAllow 这里并没有启用Kubernetes 1.5的新特性即kubelet API的认证授权功能，先设置AlwaysAllow和我们的线上环境保持一致

systemctl status -l kubelet

kubectl get nodes
NAME       STATUS     AGE       VERSION
k8s-m138   NotReady   1m        v1.6.2

#### kube-proxy部署

```sh

# 生成kubeconfig文件kube-proxy.conf：

cd /etc/kubernetes

export KUBE_APISERVER="https://192.168.200.137:8443"

# set-cluster
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/pki/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=kube-proxy.conf
  
# set-credentials
kubectl config set-credentials system:kube-proxy \
  --client-certificate=/etc/kubernetes/pki/kube-proxy.pem \
  --embed-certs=true \
  --client-key=/etc/kubernetes/pki/kube-proxy-key.pem \
  --kubeconfig=kube-proxy.conf
  
# set-context
kubectl config set-context system:kube-proxy@kubernetes \
  --cluster=kubernetes \
  --user=system:kube-proxy \
  --kubeconfig=kube-proxy.conf
  
# set default context
kubectl config use-context system:kube-proxy@kubernetes --kubeconfig=kube-proxy.conf
```

```sh
# 创建kubelet的工作目录：

mkdir -p /var/lib/kube-proxy

# 创建kube-proxy的systemd unit service文件，注意替换NodeIP变量：

export NODE_IP=`hostname -i`

tee > /usr/lib/systemd/system/kube-proxy.service <<-'EOF'
[Unit]
Description=kube-proxy
After=network.target
Requires=network.target

[Service]
WorkingDirectory=/var/lib/kube-proxy
EnvironmentFile=-/etc/kubernetes/kube-proxy
ExecStart=/usr/local/bin/kube-proxy \
    --logtostderr=true \
    --v=0 \
    --bind-address=@NODE_IP \
    --kubeconfig=/etc/kubernetes/kube-proxy.conf \
    --cluster-cidr=10.244.0.0/16        
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOF

sed -i "s#@NODE_IP#$NODE_IP#g" /usr/lib/systemd/system/kube-proxy.service

systemctl daemon-reload
systemctl enable kube-proxy
systemctl stop kube-proxy
systemctl start kube-proxy
systemctl status -l kube-proxy

# 查看服务器日志（如有报错发生）
tail -f /var/log/messages 

```

#### 部署Pod Network插件flannel

flannel以DaemonSet的形式运行在Kubernetes集群中。 由于我们的etcd集群启用了TLS认证，为了从flannel容器中能访问etcd，我们先把etcd的TLS证书信息保存到Kubernetes的Secret中。

kubectl create secret generic etcd-tls-secret  \
    --from-file=/etc/etcd/ssl/etcd.pem \
    --from-file=/etc/etcd/ssl/etcd-key.pem  \
    --from-file=/etc/etcd/ssl/ca.pem \
    -n kube-system


kubectl describe secret etcd-tls-secret -n kube-system

mkdir -p /home/k8s/flannel
cd /home/k8s/flannel
wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

对kube-flannel.yml做以下修改：

flanneld的启动参数中加入以下参数
-etcd-endpoints配置etcd集群的访问地址
-etcd-cafile配置etcd的CA证书,/etc/etcd/ssl/ca.pem从etcd-tls-secret这个Secret挂载
--etcd-certfile配置etcd的公钥证书,/etc/etcd/ssl/etcd.pem从etcd-tls-secret这个Secret挂载
--etcd-keyfile配置etcd的私钥,/etc/etcd/ssl/etcd-key.pem从etcd-tls-secret这个Secret挂载
--iface当Node节点有多个网卡时用于指明具体的网卡名称

kubectl create -f kube-flannel.yml

kubectl get pods --all-namespaces

ifconfig flannel.1

#### 部署kube-dns插件

Kubernetes支持kube-dns以Cluster Add-On的形式运行。Kubernetes会在集群中调度一个DNS的Pod和Service。
地址：https://github.com/kubernetes/kubernetes/tree/master/cluster/addons

mkdir -p /home/k8s/kube-dns
cd /home/k8s/kube-dns

wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/kube-dns.yaml.base
wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/transforms2sed.sed

查看transforms2sed.sed：

s/__PILLAR__DNS__SERVER__/$DNS_SERVER_IP/g
s/__PILLAR__DNS__DOMAIN__/$DNS_DOMAIN/g
s/__MACHINE_GENERATED_WARNING__/Warning: This is a file generated from the base underscore template file: __SOURCE_FILENAME__/g

将$DNS_SERVER_IP替换成10.96.0.10，将DNS_DOMAIN替换成cluster.local。 注意$DNS_SERVER_IP要和kubelet设置的--cluster-dns参数一致

sed -f transforms2sed.sed kube-dns.yaml.base > kube-dns.yaml

kubectl create -f kube-dns.yaml

kubectl get pods --all-namespaces

kubectl get events --all-namespaces

#### 机器清理(可选)

```sh
kubeadm reset
```
---

#### kubeadm初始化

在master1上设置kubectl的环境变量`KUBECONFIG`：

<div style="color:red;">
  以下这一步很重要，本人在虚拟机上全新安装时遇到执行完初始化以后用 kubectl get nodes 报错说连不上 api，查看集群信息 kubectl cluster-info 显示 api 地址为 http://localhost:8080 ，开始以为是网络 cni 的问题，后来才发现是没有生效下面的环境变量，取的安装自带的配置而不是 kubeadm 初始化生成的配置导致。
</div>

```sh
echo "export KUBECONFIG=/etc/kubernetes/admin.conf" >> ~/.bashrc
source ~/.bashrc
```

这里使用配置文件的高级方式初始化master，更多资料可以参考：[config-file](https://kubernetes.io/docs/admin/kubeadm/#config-file)

```sh
tee /root/kubeadm-init-v1.7.6.yaml <<-'EOF'
apiVersion: kubeadm.k8s.io/v1alpha1
kind: MasterConfiguration
api:
  advertiseAddress: 0.0.0.0
  bindPort: 61383
etcd:
  endpoints:
  - http://192.168.200.138:2379
  - http://192.168.200.139:2379
  - http://192.168.200.140:2379
networking:
  dnsDomain: 192.168.200.132
  serviceSubnet: 10.96.0.0/12
  podSubnet: 192.168.0.0/16
kubernetesVersion: v1.7.6
apiServerCertSANs:
- k8s-m138
- k8s-m139
- k8s-m140
- 127.0.0.1
- 192.168.0.1
- 10.96.0.1
- 192.168.200.138
- 192.168.200.139
- 192.168.200.140
- 192.168.200.137
EOF
```

在master1上使用kubeadm初始化kubernetes集群，连接外部etcd集群

```sh
$ kubeadm init --config=/root/kubeadm-init-v1.7.6.yaml
[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
[init] Using Kubernetes version: v1.7.6
[init] Using Authorization modes: [Node RBAC]
[preflight] Running pre-flight checks
[preflight] Starting the kubelet service
[kubeadm] WARNING: starting in 1.8, tokens expire after 24 hours by default (if you require a non-expiring token use --token-ttl 0)
[certificates] Generated CA certificate and key.
[certificates] Generated API server certificate and key.
[certificates] API Server serving cert is signed for DNS names [k8s-m138 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.192.168.200.132 k8s-m138 k8s-m139 k8s-m140] and IPs [127.0.0.1 192.168.0.1 10.96.0.1 192.168.200.138 192.168.200.139 192.168.200.140 192.168.200.137 10.96.0.1 192.168.200.138]
[certificates] Generated API server kubelet client certificate and key.
[certificates] Generated service account token signing key and public key.
[certificates] Generated front-proxy CA certificate and key.
[certificates] Generated front-proxy client certificate and key.
[certificates] Valid certificates and keys now exist in "/etc/kubernetes/pki"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/scheduler.conf"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/admin.conf"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/kubelet.conf"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/controller-manager.conf"
[apiclient] Created API client, waiting for the control plane to become ready
[apiclient] All control plane components are healthy after 31.002131 seconds
[token] Using token: 0ae401.fa0fdf56d19ffb6f
[apiconfig] Created RBAC rules
[addons] Applied essential addon: kube-proxy
[addons] Applied essential addon: kube-dns

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run (as a regular user):

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  http://kubernetes.io/docs/admin/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join --token 0ae401.fa0fdf56d19ffb6f 192.168.200.138:61383

$ rm -rf /root/kubeadm-init-v1.7.6.yaml
```

在master1上修改kube-apiserver.yaml的admission-control，v1.7.x使用了NodeRestriction等安全检查控制，务必设置成v1.6.x推荐的admission-control配置：

``` sh
sed -i 's/Initializers,//g' /etc/kubernetes/manifests/kube-apiserver.yaml
sed -i 's/NodeRestriction,//g' /etc/kubernetes/manifests/kube-apiserver.yaml  
```

在master1上重启docker kubelet服务

```sh
systemctl restart docker kubelet
```

---

#### k8s与 harbor 集成

主要是让K8s调度发布应用时可以正常拉取对象，需要建立 secret 。

```sh
# 创建 secret
$ kubectl create secret docker-registry reg.blf1.org \
  --docker-server=reg.blf1.org --docker-username=admin \
  --docker-password=dockerMan2017 --docker-email=leo.admin@wonhigh.cn

# kube-system 系统命名空间专用
$ kubectl create secret docker-registry reg.blf1.org --namespace=kube-system \
  --docker-server=reg.blf1.org --docker-username=admin \
  --docker-password=dockerMan2017 --docker-email=leo.admin@wonhigh.cn

$ kubectl get secret --all-namespaces | grep reg.blf1.org
default       reg.blf1.org                             kubernetes.io/dockercfg               1         3s
kube-system   reg.blf1.org                             kubernetes.io/dockercfg               1         8d
```

---


#### calico网络组件安装

在master1上安装calico网络组件（必须安装网络组件，否则kube-dns pod会一直处于ContainerCreating）。

```sh
$ kubectl apply -f http://down.belle.cn/package/kubernetes/v1.7.6/calico.yaml
```

官方路径安装方式：

```sh
$ kubectl apply -f https://docs.projectcalico.org/v2.6/getting-started/kubernetes/installation/hosted/kubeadm/1.6/calico.yaml
```

在master1上验证kube-dns成功启动，大概等待3分钟，验证所有pods的状态为Running。

```sh
$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                       READY     STATUS    RESTARTS   AGE       IP               NODE
kube-system   calico-etcd-plxvk                          1/1       Running   0          1m        192.168.200.138     k8s-m138
kube-system   calico-node-tvn6r                          2/2       Running   0          1m        192.168.200.138     k8s-m138
kube-system   calico-policy-controller-336633499-j394h   1/1       Running   0          1m        192.168.200.138     k8s-m138
kube-system   kube-apiserver-k8s-m138                     1/1       Running   1          4m        192.168.200.138     k8s-m138
kube-system   kube-controller-manager-k8s-m138            1/1       Running   2          13m       192.168.200.138     k8s-m138
kube-system   kube-dns-2881600278-490k4                  3/3       Running   0          14m       192.168.130.68   k8s-m138
kube-system   kube-proxy-xsfc3                           1/1       Running   1          14m       192.168.200.138     k8s-m138
kube-system   kube-scheduler-k8s-m138                     1/1       Running   1          13m       192.168.200.138     k8s-m138
```

---

#### dashboard组件安装

在master1上安装dashboard组件。

注意：这里的yaml文件为本人从官网下载，并做了些许小调整，其实完全就可以用官网提供的即可，详情参见官网：[Installing Addons](https://kubernetes.io/docs/concepts/cluster-administration/addons/)。

```sh
$ kubectl create -f http://down.belle.cn/package/kubernetes/v1.7.6/kube-dashboard.yaml
serviceaccount "kubernetes-dashboard" created
clusterrolebinding "kubernetes-dashboard" created
deployment "kubernetes-dashboard" created
service "kubernetes-dashboard" created

$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                       READY     STATUS    RESTARTS   AGE       IP               NODE
kube-system   calico-etcd-plxvk                          1/1       Running   0          7m        192.168.200.138     k8s-m138
kube-system   calico-node-tvn6r                          2/2       Running   0          7m        192.168.200.138     k8s-m138
kube-system   calico-policy-controller-336633499-j394h   1/1       Running   0          7m        192.168.200.138     k8s-m138
kube-system   kube-apiserver-k8s-m138                     1/1       Running   1          11m       192.168.200.138     k8s-m138
kube-system   kube-controller-manager-k8s-m138            1/1       Running   2          20m       192.168.200.138     k8s-m138
kube-system   kube-dns-2881600278-490k4                  3/3       Running   0          21m       192.168.130.68   k8s-m138
kube-system   kube-proxy-xsfc3                           1/1       Running   1          21m       192.168.200.138     k8s-m138
kube-system   kube-scheduler-k8s-m138                     1/1       Running   1          20m       192.168.200.138     k8s-m138
kube-system   kubernetes-dashboard-37114000809-j6tft      1/1       Running   0          1m        192.168.130.70   k8s-m138
```

部署完成后访问dashboard地址:`http://192.168.200.138:30080`，验证dashboard成功启动。

![dashboard](/assets/2017-09-29-kubeadm-ha-v1.76/dashboard.png)

至此，第一台master成功安装，并已经完成calico、dashboard。

---

### master集群高可用设置

#### 复制配置

在master1上把/etc/kubernetes/复制到master2、master3：

```sh
scp -r /etc/kubernetes/ 192.168.200.139:/etc/
scp -r /etc/kubernetes/ 192.168.200.140:/etc/
```

在master2、master3上重启kubelet服务，并检查kubelet服务状态为active (running)：

```sh
$ systemctl daemon-reload && systemctl restart kubelet

$ systemctl status kubelet
● kubelet.service - kubelet: The Kubernetes Node Agent
   Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: disabled)
  Drop-In: /etc/systemd/system/kubelet.service.d
           └─10-kubeadm.conf
   Active: active (running) since Tue 2017-06-27 16:24:22 CST; 1 day 17h ago
     Docs: http://kubernetes.io/docs/
 Main PID: 2780 (kubelet)
   Memory: 92.9M
   CGroup: /system.slice/kubelet.service
           ├─2780 /usr/bin/kubelet --kubeconfig=/etc/kubernetes/kubelet.conf --require-...
           └─2811 journalctl -k -f
```

在master2、master3上设置kubectl的环境变量`KUBECONFIG`：

```sh
echo "export KUBECONFIG=/etc/kubernetes/admin.conf" >> ~/.bashrc
source ~/.bashrc
```

在master2、master3检测节点状态，发现节点已经加进来：

```sh
$ kubectl get node -o=wide              
NAME      STATUS    AGE       VERSION   EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION
k8s-m138   Ready     33m       v1.7.5    <none>        CentOS Linux 7 (Core)   3.10.0-327.el7.x86_64
k8s-m139   Ready     2m        v1.7.5    <none>        CentOS Linux 7 (Core)   3.10.0-327.el7.x86_64
k8s-m140   Ready     140s       v1.7.5    <none>        CentOS Linux 7 (Core)   3.10.0-327.el7.x86_64

$ kubectl get node --show-labels
NAME      STATUS    AGE       VERSION   LABELS
k8s-m138   Ready     34m       v1.7.5    beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=k8s-m138,node-role.kubernetes.io/master=
k8s-m139   Ready     3m        v1.7.5    beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=k8s-m139
k8s-m140   Ready     1m        v1.7.5    beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=k8s-m140

$ kubectl get pods --all-namespaces -o wide 
NAMESPACE     NAME                                       READY     STATUS    RESTARTS   AGE       IP               NODE
kube-system   calico-etcd-plxvk                          1/1       Running   0          25m       192.168.200.138     k8s-m138
kube-system   calico-node-glmmp                          2/2       Running   1          8m        192.168.200.139     k8s-m139
kube-system   calico-node-tvn6r                          2/2       Running   0          25m       192.168.200.138     k8s-m138
kube-system   calico-node-vvjmr                          2/2       Running   1          6m        192.168.200.140     k8s-m140
kube-system   calico-policy-controller-336633499-j394h   1/1       Running   0          25m       192.168.200.138     k8s-m138
kube-system   kube-apiserver-k8s-m138                     1/1       Running   1          29m       192.168.200.138     k8s-m138
kube-system   kube-apiserver-k8s-m139                     1/1       Running   0          8m        192.168.200.139     k8s-m139
kube-system   kube-apiserver-k8s-m140                     1/1       Running   0          6m        192.168.200.140     k8s-m140
kube-system   kube-controller-manager-k8s-m138            1/1       Running   2          38m       192.168.200.138     k8s-m138
kube-system   kube-controller-manager-k8s-m139            1/1       Running   0          8m        192.168.200.139     k8s-m139
kube-system   kube-controller-manager-k8s-m140            1/1       Running   0          6m        192.168.200.140     k8s-m140
kube-system   kube-dns-2881600278-490k4                  3/3       Running   0          38m       192.168.130.68   k8s-m138
kube-system   kube-proxy-1pzjr                           1/1       Running   0          6m        192.168.200.140     k8s-m140
kube-system   kube-proxy-qfssw                           1/1       Running   0          8m        192.168.200.139     k8s-m139
kube-system   kube-proxy-xsfc3                           1/1       Running   1          38m       192.168.200.138     k8s-m138
kube-system   kube-scheduler-k8s-m138                     1/1       Running   1          38m       192.168.200.138     k8s-m138
kube-system   kube-scheduler-k8s-m139                     1/1       Running   0          8m        192.168.200.139     k8s-m139
kube-system   kube-scheduler-k8s-m140                     1/1       Running   0          6m        192.168.200.140     k8s-m140
kube-system   kubernetes-dashboard-37114000809-j6tft      1/1       Running   0          19m       192.168.130.70   k8s-m138
```

---

#### 修改配置

在master2上修改以下配置，改为k8s-m139的IP

```sh
sed -i 's/=192.168.200.138/=192.168.200.139/g' /etc/kubernetes/manifests/kube-apiserver.yaml
sed -i 's/192.168.200.138/192.168.200.139/g' /etc/kubernetes/kubelet.conf
sed -i 's/192.168.200.138/192.168.200.139/g' /etc/kubernetes/admin.conf
sed -i 's/192.168.200.138/192.168.200.139/g' /etc/kubernetes/controller-manager.conf
sed -i 's/192.168.200.138/192.168.200.139/g' /etc/kubernetes/scheduler.conf
```

在master3上修改以下配置，改为k8s-m140的IP

```sh
sed -i 's/=192.168.200.138/=192.168.200.140/g' /etc/kubernetes/manifests/kube-apiserver.yaml
sed -i 's/192.168.200.138/192.168.200.140/g' /etc/kubernetes/kubelet.conf
sed -i 's/192.168.200.138/192.168.200.140/g' /etc/kubernetes/admin.conf
sed -i 's/192.168.200.138/192.168.200.140/g' /etc/kubernetes/controller-manager.conf
sed -i 's/192.168.200.138/192.168.200.140/g' /etc/kubernetes/scheduler.conf
```

在master1、master2、master3上重启所有服务：

```sh
$ systemctl daemon-reload && systemctl restart docker kubelet
```

在master1、master2、master3任意节点上检测服务启动情况，发现kube-apiserver、kube-controller-manager、kube-scheduler、kube-proxy、calico-node已经在master1、master2、master3成功启动。

```sh
$ kubectl get pods --all-namespaces -o wide | grep k8s-m140
kube-system   calico-node-vvjmr                          2/2       Running   3          21m       192.168.200.140     k8s-m140
kube-system   kube-apiserver-k8s-m140                     1/1       Running   1          3m        192.168.200.140     k8s-m140
kube-system   kube-controller-manager-k8s-m140            1/1       Running   1          21m       192.168.200.140     k8s-m140
kube-system   kube-proxy-1pzjr                           1/1       Running   1          21m       192.168.200.140     k8s-m140
kube-system   kube-scheduler-k8s-m140                     1/1       Running   1          21m       192.168.200.140     k8s-m140

$ kubectl get pods --all-namespaces -o wide | grep k8s-m139
kube-system   calico-node-glmmp                          2/2       Running   3          24m       192.168.200.139     k8s-m139
kube-system   kube-apiserver-k8s-m139                     1/1       Running   0          1m        192.168.200.139     k8s-m139
kube-system   kube-controller-manager-k8s-m139            1/1       Running   1          24m       192.168.200.139     k8s-m139
kube-system   kube-proxy-qfssw                           1/1       Running   1          24m       192.168.200.139     k8s-m139
kube-system   kube-scheduler-k8s-m139                     1/1       Running   1          24m       192.168.200.139     k8s-m139
```

---

#### 组件扩容

在master1、master2、master3任意节点上查看deployment的情况：

```sh
$ kubectl get deploy --all-namespaces
NAMESPACE     NAME                       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kube-system   calico-policy-controller   1         1         1            1           4d
kube-system   kube-dns                   1         1         1            1           4d
kube-system   kubernetes-dashboard       1         1         1            1           4d
```

在master1、master2、master3任意节点上把 kubernetes-dashboard、kube-dns 扩容成replicas=3，保证各个master节点上都有运行：

```sh
$ kubectl scale --replicas=3 -n kube-system deployment/kube-dns
deployment "kube-dns" scaled

$ kubectl get pods --all-namespaces -o wide| grep kube-dns
kube-system   kube-dns-2881600278-1hfml                  3/3       Running   0          42s       192.168.137.1     k8s-m140
kube-system   kube-dns-2881600278-490k4                  3/3       Running   0          6d        192.168.130.66    k8s-m138
kube-system   kube-dns-2881600278-dfw13                  3/3       Running   0          42s       192.168.169.129   k8s-m139

$ kubectl get pods --all-namespaces -o wide| grep kube-dns
kube-system   kube-dns-2881600278-490k4                  3/3       Running   0          4d        192.168.130.68   k8s-m138

$ kubectl scale --replicas=3 -n kube-system deployment/kubernetes-dashboard
deployment "kubernetes-dashboard" scaled

$ kubectl get pods --all-namespaces -o wide| grep kubernetes-dashboard
kube-system   kubernetes-dashboard-37114000809-2t954      1/1       Running   0          14s       192.168.137.2     k8s-m140
kube-system   kubernetes-dashboard-37114000809-8s0tw      1/1       Running   0          14s       192.168.169.146   k8s-m139
kube-system   kubernetes-dashboard-37114000809-j6tft      1/1       Running   0          4d        192.168.130.70    k8s-m138

$ kubectl get pods --all-namespaces -o wide| grep calico-policy-controller
kube-system   calico-policy-controller-336633499-j394h   1/1       Running   0          4d        192.168.200.138      k8s-m138

$ kubectl scale --replicas=3 -n kube-system deployment/calico-policy-controller
deployment "calico-policy-controller" scaled

$ kubectl get pods --all-namespaces -o wide| grep calico-policy-controller
kube-system   calico-policy-controller-336633499-j394h   1/1       Running   0          4d        192.168.200.138      k8s-m138
kube-system   calico-policy-controller-336633499-p2vw5   1/1       Running   0          32s       192.168.200.139      k8s-m139
kube-system   calico-policy-controller-336633499-tw2qj   1/1       Running   0          32s       192.168.200.140      k8s-m140
```

---


---


#### kube-proxy配置vip

在master1上设置kube-proxy使用keepalived的虚拟IP地址，避免master1异常的时候所有节点的kube-proxy连接不上

```sh
$ kubectl get -n kube-system configmap
NAME                                 DATA      AGE
calico-config                        3         5d
extension-apiserver-authentication   6         5d
kube-proxy                           1         5d
```

在master1上修改configmap/kube-proxy的server指向keepalived的虚拟IP地址

```sh
$ kubectl edit -n kube-system configmap/kube-proxy
        server: https://192.168.200.137:61383

$ kubectl get -n kube-system configmap/kube-proxy -o yaml
```

在master1上删除所有kube-proxy的pod，让proxy重建

```
$ kubectl get pods --all-namespaces -o wide | grep proxy 
kube-system   kube-proxy-1pzjr                           1/1       Running   1          5d        192.168.200.140      k8s-m140
kube-system   kube-proxy-qfssw                           1/1       Running   1          5d        192.168.200.139      k8s-m139
kube-system   kube-proxy-xsfc3                           1/1       Running   1          5d        192.168.200.138      k8s-m138

$ kubectl delete pod kube-proxy-1pzjr --namespace=kube-system
pod "kube-proxy-1pzjr" deleted

$ kubectl delete pod kube-proxy-qfssw --namespace=kube-system
pod "kube-proxy-qfssw" deleted

$ kubectl delete pod kube-proxy-xsfc3 --namespace=kube-system
pod "kube-proxy-xsfc3" deleted

$ kubectl get pods --all-namespaces -o wide | grep proxy 
kube-system   kube-proxy-36tg0                           1/1       Running   0          15s       192.168.200.139      k8s-m139
kube-system   kube-proxy-46t3m                           1/1       Running   0          24s       192.168.200.140      k8s-m140
kube-system   kube-proxy-q4bhn                           1/1       Running   0          7s        192.168.200.138      k8s-m138
```

在master1、master2、master3上重启docker kubelet keepalived服务

```sh
$ systemctl restart docker kubelet keepalived
```

---

#### calico配置调整

调整calico使用外部的 etcd，消除单点故障。

主要调整有2块：

* 删掉canal.yaml中关于etcd的部署代码

* 修改`etcd_endpoints`为已部署的etcd集群

```sh
$ kubectl delete -f http://down.belle.cn/package/kubernetes/v1.7.6/calico.yaml

$ kubectl apply -f http://down.belle.cn/package/kubernetes/v1.7.6/calico_external_etcd.yaml

$ kubectl get pods --all-namespaces -o wide| grep calico-policy-controller
kube-system   kubernetes-dashboard-37114000809-wncnc      1/1       Running   0          9s        192.168.130.84    k8s-m138
```

这里要注意下，calico-policy-controller是否在master1上发布成功，如果发布到其他的节点，可能会导致无法创建，这时需要重复上面的步骤，直到发布到master1上即可，报错如下：

```sh 
User "system:node:k8s-m138" cannot get secrets in the namespace "kube-system".: "no path found to object" (get secrets calico-policy-controller-token-x079r)
```

具体原因在后续再跟进。

calico-policy-controller发布成功后执行扩容：

```sh
$ kubectl scale --replicas=3 -n kube-system deployment/calico-policy-controller

$ kubectl get pods --all-namespaces -o wide| grep calico-policy-controller
kube-system   kubernetes-dashboard-37114000809-6p70t      1/1       Running   0          9s        192.168.137.12    k8s-m140
kube-system   kubernetes-dashboard-37114000809-k2lpj      1/1       Running   0          11s       192.168.169.142   k8s-m139
kube-system   kubernetes-dashboard-37114000809-wncnc      1/1       Running   0          9s        192.168.130.84    k8s-m138
```

---


#### 查看master集群高可用

在master1上检查各个节点pod的启动状态，每个上都成功启动kube-apiserver、kube-controller-manager、kube-dns、kube-proxy、kube-scheduler、kubernetes-dashboard、calico。并且所有pod都处于Running状态表示正常。

```
$ kubectl get pods --all-namespaces -o wide | grep k8s-m138

$ kubectl get pods --all-namespaces -o wide | grep k8s-m139

$ kubectl get pods --all-namespaces -o wide | grep k8s-m140
```

---

#### 禁用master发布应用

在master1上禁止在所有master节点上发布应用。（可选项，个人认为没有太大的必要）

```
$ kubectl patch node k8s-m138 -p '{"spec":{"unschedulable":true}}'

$ kubectl patch node k8s-m139 -p '{"spec":{"unschedulable":true}}'

$ kubectl patch node k8s-m140 -p '{"spec":{"unschedulable":true}}'
```

---

#### node节点加入集群

在master1上查看集群的token

```sh
$ kubeadm token list
TOKEN                     TTL         EXPIRES   USAGES                   DESCRIPTION
4bd22d.62d81397a9558abbb   <forever>   <never>   authentication,signing   The default bootstrap token generated by 'kubeadm init'.
```

在zabbix-46上执行：

```sh
$ kubeadm join --token 4bd22d.62d81397a9558abbb 192.168.200.137:81383

$ systemctl enable kubelet && systemctl restart kubelet && systemctl status kubelet
```

启动成功后，在master1查看节点：

```sh
$ kubectl get nodes 
NAME        STATUS    AGE       VERSION
k8s-m138     Ready     14h       v1.7.5
k8s-m139     Ready     14h       v1.7.5
k8s-m140     Ready     14h       v1.7.5
zabbix-46   Ready     1m        v1.7.5
```

---

#### 部署应用验证集群

发布一个测试的nginx应用，可以正常访问：`http://192.168.200.137:30800/`即说明成功。

```sh
$ kubectl create -f http://down.belle.cn/package/kubernetes/v1.7.6/test_cluster/app_namespace.yaml

$ kubectl create -f http://down.belle.cn/package/kubernetes/v1.7.6/test_cluster/app_deploy_rc.yaml

$ kubectl create -f http://down.belle.cn/package/kubernetes/v1.7.6/test_cluster/app_deploy_service.yaml

$ kubectl get pods --all-namespaces -o=wide  | grep nginx
dev-web       nginx.1.10.1-flrrt                         1/1       Running   0          34s       192.168.224.129   zabbix-46
```

app_namespace.yaml

```sh
kind: Namespace
apiVersion: v1
metadata:
  name: dev-web
```

app_deploy_rc.yaml

```sh
kind: ReplicationController
apiVersion: v1
metadata:
  name: nginx.1.10.1
  namespace: dev-web
  labels:
    name: nginx
    version: "1.10.1"
spec:
  replicas: 1
  selector:
    name: nginx
    version: "1.10.1"
  template:
    metadata:
      labels:
        name: nginx
        version: "1.10.1"
    spec:
      imagePullSecrets:
      - name: reg.blf1.org
      containers:
      - name: nginx
        image: reg.blf1.org/tools/nginx:1.10.1
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
        env:
        - name: TZ
          value: "Asia/Shanghai"
        command: ["nginx", "-g", "daemon off;"]
        resources:
          requests:
            cpu: 50m
            memory: 50Mi
          limits:
            cpu: 100m
            memory: 100Mi
```

app_deploy_service.yaml

```sh
apiVersion: v1
kind: Service
metadata:
  name: nginx
  namespace: dev-web
  labels:
    name: nginx
spec:
  type: NodePort
  ports:
  - port: 80
    name: container-port
    nodePort: 30800
  selector:
    name: nginx
```
---

### 其他有用资料

#### 限制上外网

设置 ipMasq 参数，备忘，有需要再验证。

docker：--ip-masq=false

cni：

cni-conf.json: |
    {
      "name": "cbr0",
      "type": "flannel",
      "ipMasq": false,
      "delegate": {
        "isDefaultGateway": true
      }
    }
    
[Kubeadm部署k8s](http://www.winseliu.com/blog/2017/08/13/kubeadm-install-k8s-on-centos7-with-resources/)

---

至此，kubernetes高可用集群成功部署。

### 参考资料

[kubernetes 1.7.3 + calico 多 Master](https://jicki.me/2017/08/08/kubernetes-1.7.3/)

[k8s kubeadm部署高可用集群](http://www.cnblogs.com/caiwenhao/p/6196014.html)

---

**转载**请注明出处，本文采用 [CC4.0](http://creativecommons.org/licenses/by-nc-nd/4.0/) 协议授权，版权归 [ん乖乖龙ん](https://bjddd192.github.io) 所有。